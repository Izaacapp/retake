"""
Advanced voice cloning using Coqui TTS, Bark, and other 2025 models
"""
import os
from pathlib import Path
from typing import Optional, List
import torch
import numpy as np


class VoiceCloner:
    """Clone voices using state-of-the-art TTS models"""
    
    def __init__(self, output_dir: str = "output/voice_models"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ðŸŽ™ï¸ Voice Cloner initialized on {self.device}")
        
        # Model will be loaded on demand
        self.tts_model = None
    
    def load_coqui_model(self, model_name: str = "tts_models/en/vctk/vits"):
        """
        Load Coqui TTS model for voice cloning
        
        Args:
            model_name: Name of the TTS model to use
        """
        try:
            from TTS.api import TTS
            print(f"ðŸ“¦ Loading Coqui TTS model: {model_name}...")
            self.tts_model = TTS(model_name=model_name, progress_bar=True).to(self.device)
            print(f"âœ… Model loaded successfully")
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            print("ðŸ’¡ Installing with: pip install TTS")
            raise
    
    def clone_voice_from_sample(
        self,
        reference_audio: str,
        speaker_name: str
    ) -> str:
        """
        Create a voice model from reference audio
        
        Args:
            reference_audio: Path to clean audio sample of the speaker
            speaker_name: Name to identify the speaker
        
        Returns:
            Path to voice model/embedding
        """
        print(f"ðŸ§¬ Cloning voice for {speaker_name}...")
        
        try:
            # For advanced voice cloning, we use embeddings
            # This is a placeholder - actual implementation would use:
            # - Resemblyzer for speaker embeddings
            # - Coqui TTS for multi-speaker voice cloning
            # - StyleTTS2 for high-quality synthesis
            
            from resemblyzer import VoiceEncoder, preprocess_wav
            
            encoder = VoiceEncoder()
            wav = preprocess_wav(Path(reference_audio))
            embed = encoder.embed_utterance(wav)
            
            # Save embedding
            embedding_path = self.output_dir / f"{speaker_name}_embedding.npy"
            np.save(str(embedding_path), embed)
            
            print(f"âœ… Voice model created: {embedding_path}")
            return str(embedding_path)
            
        except ImportError:
            print("âš ï¸ Resemblyzer not available, using simple mode")
            # Return reference audio as fallback
            return reference_audio
        except Exception as e:
            print(f"âŒ Error in voice cloning: {e}")
            raise
    
    def synthesize_speech(
        self,
        text: str,
        speaker_model: str,
        output_filename: str,
        speed: float = 1.0,
        emotion: str = "neutral"
    ) -> str:
        """
        Generate speech from text using cloned voice
        
        Args:
            text: Text to synthesize
            speaker_model: Path to speaker model/embedding
            output_filename: Name for output audio file
            speed: Speaking speed multiplier
            emotion: Emotion style (neutral, happy, serious, etc.)
        
        Returns:
            Path to generated audio file
        """
        print(f"ðŸ—£ï¸ Synthesizing: '{text[:50]}...'")
        
        try:
            if self.tts_model is None:
                self.load_coqui_model()
            
            output_path = self.output_dir / f"{output_filename}.wav"
            
            # Generate speech
            # Note: This is simplified - actual implementation depends on the model
            if hasattr(self.tts_model, 'tts_to_file'):
                self.tts_model.tts_to_file(
                    text=text,
                    file_path=str(output_path),
                    speaker_wav=speaker_model if Path(speaker_model).exists() else None
                )
            else:
                # Fallback method
                wav = self.tts_model.tts(text=text)
                import soundfile as sf
                sf.write(str(output_path), wav, 22050)
            
            print(f"âœ… Speech generated: {output_path}")
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ Error in speech synthesis: {e}")
            raise
    
    def batch_synthesize(
        self,
        script_segments: List[dict],
        speaker_model: str,
        prefix: str = "segment"
    ) -> List[str]:
        """
        Synthesize multiple text segments
        
        Args:
            script_segments: List of dicts with 'text' and optional 'id'
            speaker_model: Path to speaker model
            prefix: Filename prefix
        
        Returns:
            List of paths to generated audio files
        """
        print(f"ðŸ“ Synthesizing {len(script_segments)} segments...")
        
        output_files = []
        for i, segment in enumerate(script_segments):
            text = segment.get('text', '')
            seg_id = segment.get('id', i)
            
            output_file = self.synthesize_speech(
                text=text,
                speaker_model=speaker_model,
                output_filename=f"{prefix}_{seg_id:03d}"
            )
            output_files.append(output_file)
        
        print(f"âœ… Generated {len(output_files)} audio files")
        return output_files
    
    def improve_prosody(
        self,
        audio_path: str,
        target_speaker: str
    ) -> str:
        """
        Improve prosody and naturalness of generated speech
        
        Args:
            audio_path: Path to generated speech audio
            target_speaker: Reference speaker for prosody
        
        Returns:
            Path to improved audio
        """
        print(f"ðŸŽ­ Improving prosody...")
        
        # This would use advanced models like StyleTTS2 or Bark
        # For now, return the original
        print(f"âœ… Prosody enhanced (placeholder)")
        return audio_path
